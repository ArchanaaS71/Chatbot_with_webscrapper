# -*- coding: utf-8 -*-
"""Webscrapped data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_8ifLcS-CsbiV1-7_nkaB-JGK2GCm-V
"""

!pip install requests beautifulsoup4

!pip install reportlab
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter

def fetch_html(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        return response.content
    except requests.exceptions.HTTPError as e:
        print(f"HTTP error occurred: {e}")
    except requests.exceptions.RequestException as e:
        print(f"An error occurred: {e}")
    return None

def parse_html(html_content):
    return BeautifulSoup(html_content, 'html.parser')

def scrape_website_and_generate_pdf(base_url, output_pdf):
    visited_urls = set()
    queue = [base_url]

    c = canvas.Canvas(output_pdf, pagesize=letter)
    y_position = 750

    while queue:
        url = queue.pop(0)

        if url in visited_urls:
            continue

        print(f"Scraping {url}")

        html_content = fetch_html(url)
        if html_content:
            visited_urls.add(url)

            soup = parse_html(html_content)

            # Example: Extracting text from paragraphs
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                paragraph_text = p.text.strip()
                print(paragraph_text)

                # Write paragraph to PDF
                c.drawString(50, y_position, paragraph_text)
                y_position -= 15

            # Extract all links from the page
            links = soup.find_all('a', href=True)
            for link in links:
                next_url = urljoin(url, link['href'])
                parsed_url = urlparse(next_url)

                # Ensure the next URL is within the same domain
                if parsed_url.netloc == urlparse(base_url).netloc and next_url not in visited_urls:
                    queue.append(next_url)

    c.save()

# Example usage
base_url = 'http://www.trimble.com'
output_pdf = 'output.pdf'
scrape_website_and_generate_pdf(base_url, output_pdf)