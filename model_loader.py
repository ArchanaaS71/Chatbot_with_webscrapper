# -*- coding: utf-8 -*-
"""model_loader

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17pUc5S8KtWQiwsHMzcROq7KbMHusmHiq
"""

"""!pip install streamlit
!pip install transformers
!pip install reportlab"""
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
import streamlit as st
from transformers import pipeline

# Fetch HTML content from a URL
def fetch_html(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        return response.content
    except requests.exceptions.HTTPError as e:
        print(f"HTTP error occurred: {e}")
    except requests.exceptions.RequestException as e:
        print(f"An error occurred: {e}")
    return None

# Parse HTML content with BeautifulSoup
def parse_html(html_content):
    return BeautifulSoup(html_content, 'html.parser')

# Clean text by removing HTML tags, non-alphanumeric characters, normalizing whitespace, and converting to lowercase
def clean_text(text):
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove non-alphanumeric characters
    text = re.sub(r'\s+', ' ', text).strip()  # Normalize whitespace
    text = text.lower()  # Convert to lowercase
    return text

# Scrape website and extract text from paragraphs
def scrape_website(base_url):
    visited_urls = set()
    queue = [base_url]
    content_list = []

    while queue:
        url = queue.pop(0)

        if url in visited_urls:
            continue

        print(f"Scraping {url}")

        html_content = fetch_html(url)
        if html_content:
            visited_urls.add(url)

            soup = parse_html(html_content)

            # Extract text from paragraphs
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                paragraph_text = p.text.strip()
                cleaned_text = clean_text(paragraph_text)
                if cleaned_text:  # Ensure we don't add empty strings
                    print(cleaned_text)
                    content_list.append(cleaned_text)

            # Extract all links from the page
            links = soup.find_all('a', href=True)
            for link in links:
                next_url = urljoin(url, link['href'])
                parsed_url = urlparse(next_url)

                # Ensure the next URL is within the same domain
                if parsed_url.netloc == urlparse(base_url).netloc and next_url not in visited_urls:
                    queue.append(next_url)

    return content_list

# Example usage
base_url = 'http://www.trimble.com'
content = scrape_website(base_url)

# Save content to PDF (optional)
def save_to_pdf(content, filename='output.pdf'):
    pdf = canvas.Canvas(filename, pagesize=letter)
    width, height = letter

    y = height - 40
    for paragraph in content:
        pdf.drawString(30, y, paragraph)
        y -= 20
        if y < 40:
            pdf.showPage()
            y = height - 40

    pdf.save()

save_to_pdf(content)

# Using a pre-trained BERT model for question answering
qa_model = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')

def get_response(question, context):
    response = qa_model(question=question, context=context)
    return response['answer']
